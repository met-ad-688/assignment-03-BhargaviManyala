---
title: Assignment 03
author:
  - name: Bhargavi Manyala
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
number-sections: true
date: today
format:
  html:
    theme: cerulean
    toc: true
    toc-depth: 2
  pdf:  
    
    toc-depth: 2
    geometry: 
      - landscape
      - margin=0.5in
date-modified: today
date-format: long
jupyter: assignment03-venv
execute:
  echo: true
  eval: true
  freeze: auto
---


# Import Dataset


```{python}
from pyspark.sql import SparkSession
import pandas as pd
import plotly.express as px
import plotly.io as pio
import numpy as np
np.random.seed(42)
pio.renderers.default = "notebook"
 
spark = SparkSession.builder.appName("LightcastData").getOrCreate()
df = (
    spark.read
    .option("header", "true")
    .option("inferSchema", "true")
    .option("multiLine", "true")
    .option("escape", "\"")  
    .csv("lightcast_job_postings.csv")
)

#df.show(5)



```


# Relational Tables 

## Loactions Table
`LOCATION_ID (Primary Key)`,`LOCATION`, `CITY_NAME`, `STATE_NAME`, `COUNTY_NAME`,`MSA`, `MSA_NAME`

```{python}
from pyspark.sql.functions import monotonically_increasing_id


locations_df = (
    df.select(
        "LOCATION",
        "CITY_NAME",
        "STATE_NAME",
        "COUNTY_NAME",
        "MSA",
        "MSA_NAME"
    )
    .distinct()
    .withColumn("LOCATION_ID", monotonically_increasing_id())
    .select(
        "LOCATION_ID",
        "LOCATION",
        "CITY_NAME",
        "STATE_NAME",
        "COUNTY_NAME",
        "MSA",
        "MSA_NAME"
    )
    .orderBy("LOCATION_ID")
)

locations_df.createOrReplaceTempView("locations")

locations = locations_df.toPandas()
locations.head()

```

## Companies Table
`COMPANY_ID (Primary Key)`, `COMPANY`, `COMPANY_NAME`, `COMPANY_RAW`, `COMPANY_IS_STAFFING`

```{python}
from pyspark.sql.functions import monotonically_increasing_id

companies_df = (
    df.select(
        "COMPANY",
        "COMPANY_NAME",
        "COMPANY_RAW",
        "COMPANY_IS_STAFFING"
    )
    .distinct()
    .withColumn("COMPANY_ID", monotonically_increasing_id())
    .select(
        "COMPANY_ID",
        "COMPANY",
        "COMPANY_NAME",
        "COMPANY_RAW",
        "COMPANY_IS_STAFFING"
    )
    .orderBy("COMPANY_ID")
)

companies_df.createOrReplaceTempView("companies")

companies = companies_df.toPandas()
companies.head(4)

```

## Industries Table

`INDUSTRY_ID (Primary Key)`, `NAICS_2022_6`, `NAICS_2022_6_NAME`, `SOC_5`, `SOC_5_NAME`,`LOT_SPECIALIZED_OCCUPATION_NAME`,`LOT_OCCUPATION_GROUP`

```{python}
from pyspark.sql.functions import monotonically_increasing_id

industries_df = (
    df.select(
        "NAICS_2022_6",
        "NAICS_2022_6_NAME",
        "SOC_5",
        "SOC_5_NAME",
        "LOT_SPECIALIZED_OCCUPATION_NAME",
        "LOT_OCCUPATION_GROUP"
    )
    .distinct()
    .withColumn("INDUSTRY_ID", monotonically_increasing_id())
    .select(
        "INDUSTRY_ID",
        "NAICS_2022_6",
        "NAICS_2022_6_NAME",
        "SOC_5",
        "SOC_5_NAME",
        "LOT_SPECIALIZED_OCCUPATION_NAME",
        "LOT_OCCUPATION_GROUP"
    )
    .orderBy("INDUSTRY_ID")
)

industries_df.createOrReplaceTempView("industries")

industries = industries_df.toPandas()
industries.head()

```

## Job Postings Table

`ID (Primary Key)`, `TITLE_CLEAN`, `COMPANY_ID (FK to companies)`, `INDUSTRY_ID (FKto industries)`, `EMPLOYMENT_TYPE_NAME`, `REMOTE_TYPE_NAME`, `BODY`,`MIN_YEARS_EXPERIENCE`, `MAX_YEARS_EXPERIENCE`, `SALARY`, `SALARY_FROM`,`SALARY_TO`, `LOCATION_ID (FK to locations)`, `POSTED`, `EXPIRED`, `DURATION`


```{python}
job_postings_df = df.select(
    "ID",
    "TITLE_CLEAN",
    "BODY",
    "COMPANY",            
    "EMPLOYMENT_TYPE_NAME",
    "REMOTE_TYPE_NAME",
    "MIN_YEARS_EXPERIENCE",
    "MAX_YEARS_EXPERIENCE",
    "SALARY",
    "SALARY_FROM",
    "SALARY_TO",
    "LOCATION",           
    "NAICS_2022_6",       
    "POSTED",
    "EXPIRED",
    "DURATION"
).withColumnRenamed("ID", "JOB_ID").orderBy("JOB_ID")
 
```

### Adding Foreign Keys to Job Postings Table

```{python}
job_postings_df = job_postings_df.join(
    companies_df.select("COMPANY", "COMPANY_ID"),
    on="COMPANY", 
    how="left"
)

# Join with Locations Table to get LOCATION_ID
job_postings_df = job_postings_df.join(
    locations_df.select("LOCATION", "LOCATION_ID"),
    job_postings_df.LOCATION == locations_df.LOCATION,
    how="left"
).drop("LOCATION")

# Join with Industries Table to get INDUSTRY_ID
job_postings_df = job_postings_df.join(
    industries_df.select("NAICS_2022_6", "INDUSTRY_ID"),
    job_postings_df.NAICS_2022_6 == industries_df.NAICS_2022_6,
    how="left"
).drop("NAICS_2022_6")

job_postings_df = job_postings_df.drop("COMPANY", "LAT-LONG")
job_postings_df.createOrReplaceTempView("job_postings")

job_postings_df.show(truncate=10)
```

#  Casting salary and experience columns

## Computing medians and Imputing missing salaries
```{python}

from pyspark.sql.functions import col

df = df.withColumn("SALARY", col("SALARY").cast("float")) \
       .withColumn("SALARY_FROM", col("SALARY_FROM").cast("float")) \
       .withColumn("SALARY_TO", col("SALARY_TO").cast("float")) \
       .withColumn("MIN_YEARS_EXPERIENCE", col("MIN_YEARS_EXPERIENCE").cast("float")) \
       .withColumn("MAX_YEARS_EXPERIENCE", col("MAX_YEARS_EXPERIENCE").cast("float"))

# Computing medians for salary columns
def compute_median(sdf, col_name):
    return sdf.approxQuantile(col_name, [0.5], 0.01)[0]

median_from = compute_median(df, "SALARY_FROM")
median_to   = compute_median(df, "SALARY_TO")
median_salary = compute_median(df, "SALARY")

print("Medians:", median_from, median_to, median_salary)

# Imputing missing salaries, but not experience
df = df.fillna({
    "SALARY_FROM": median_from,
    "SALARY_TO": median_to,
    "SALARY": median_salary
})

# Computing average salary
df = df.withColumn(
    "Average_Salary", (col("SALARY_FROM") + col("SALARY_TO")) / 2
)

# Selecting required columns
export_cols = [
    "Average_Salary",
    "SALARY",
    "EDUCATION_LEVELS_NAME",
    "REMOTE_TYPE_NAME",
    "MAX_YEARS_EXPERIENCE",
    "LOT_V6_SPECIALIZED_OCCUPATION_NAME"
]

df_selected = df.select(*export_cols)

pdf_selected = df_selected.toPandas()
pdf_selected.to_csv("data/lightcast_cleaned.csv", index=False)
pdf_selected.head()

```

## Cleaning Education column and  Exporting Cleaned Data
